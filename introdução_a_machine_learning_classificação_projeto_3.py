# -*- coding: utf-8 -*-
"""Introdução a Machine Learning Classificação projeto 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dZZ4BaQhqvM9e9g92rOpLTdrg-6y3zso
"""

import pandas as pd

uri = "https://gist.githubusercontent.com/guilhermesilveira/1b7d5475863c15f484ac495bd70975cf/raw/16aff7a0aee67e7c100a2a48b676a2d2d142f646/projects.csv"
dados = pd.read_csv(uri)
dados.head()

a_renomear = {
    
    "expected_hours":"horas_esperadas",
    "price":"preco",
    "unfinished":"nao_finalizado"

}

dados = dados.rename(columns=a_renomear)
dados.head(50)

troca = {  
    0:1,
    1:0
    }#Invertendo os valores na coluna, ou seja, onde era 0 agora será 1 e onde era 1 será 0.

dados["finalizado"] = dados.nao_finalizado.map(troca)#Criando nova coluna e mapeando o dicionario troca. 
dados.head()

dados.tail() #Verificando o final dos dados (tabela)

#voltar nos 7 minuots do video.
import seaborn as sns 

sns.scatterplot(x="horas_esperadas", y="preco", data=dados)

sns.scatterplot(x="horas_esperadas", y="preco",hue="finalizado",data=dados)# "hue" - distinção de cor

sns.relplot(x="horas_esperadas", y="preco", hue="finalizado", col="finalizado",data=dados)# "hue" - distinção de cor - Plot relativo. Coluna de plots diferentes

x = dados[['horas_esperadas','preco']]
y = dados['finalizado']

from numpy as np
from sklearn.model_selection import train_test_split #Separando o treino e teste de um conjunto de dados qualquer
from sklearn.svm import LinearSVC #estimador
from sklearn.metrics import accuracy_score
import numpy as np


SEED = 5  #Definindo a ordem dos números aleatórios
np.random.seed(SEED)
#random_stateint, RandomState instance or None, default=None -> Controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls.
treino_x, teste_x, treino_y, teste_y = train_test_split(x, y, test_size = 0.25,stratify = y) #Separando os dados para treino e teste e estipulando o tamanho para o teste, também. 
print("Treinaremos com %d elementos e testaremos com %d elementos" % (len(treino_x),len(teste_x)))

modelo = LinearSVC()
modelo.fit(treino_x, treino_y)

previsoes = modelo.predict(teste_x) #Predizendo através dos meus dados de teste, após ser treinado com meus dados de treino
taxa_de_acerto = accuracy_score(teste_y, previsoes)* 100 #Imprimir minha taxa de acerto ou "acurácia" - Comparação com os meus dados reais de teste
print("A acurácia foi %.2f" % taxa_de_acerto)

"""Previsoes de base - Linha de base para o meu modelo (superar) """

import numpy as np
previsoes_de_base = np.ones(540) #gerando dados para compração. 
taxa_de_acerto = accuracy_score(teste_y, previsoes_de_base)* 100 #Imprimir minha taxa de acerto ou "acurácia" - Comparação com os meus dados reais de teste
print("A acurácia do algoritmo de baseline foi %.2f" % taxa_de_acerto)

sns.scatterplot(x="horas_esperadas", y="preco", hue=teste_y, data=teste_x) #teste_y -> Dados que eu quero seguir para cada cor

x_min = teste_x.horas_esperadas.min()
x_max = teste_x.horas_esperadas.max()
y_min = teste_x.preco.min()
y_max = teste_x.preco.max()
print(x_min, x_max, y_min, y_max)

pixels = 100
eixo_x = np.arange(x_min, x_max, (x_max - x_min)/pixels)
eixo_y = np.arange(x_min, x_max, (x_max - x_min)/pixels)

xx, yy = np.meshgrid(eixo_x, eixo_y) #Return coordinate matrices from coordinate vectors.Ex.: Plano cartesiano
pontos = np.c_[xx.ravel(), yy.ravel()]#Return a contiguous flattened array
pontos

Z = modelo.predict(pontos) #Prevendo para todos os pontos 
Z = Z.reshape(xx.shape)

import matplotlib.pyplot as plt
plt.contourf(xx, yy, Z, alpha=0.3) #Desenhar o contorno de xx e yy baseado nos pontos de Z
plt.scatter(teste_x.horas_esperadas, teste_x.preco, c=teste_y, s=5)

plt.contourf(xx, yy, Z, alpha=0.3)
plt.scatter(teste_x.horas_esperadas, teste_x.preco, c=teste_y, s=1)

#Decision Boundary - Borda de decisão

"""Nesse caso, o algoritmo usado é capaz de aprender apenas relacionamentos lineares, ou seja, em linha reta. Pra conseguirmos ter uma boa borda de decisão, precisaremos de algoritmos que sejam capazes de aprender outros tipos de relacionamentos."""

from sklearn.model_selection import train_test_split #Separando o treino e teste de um conjunto de dados qualquer
from sklearn.svm import SVC #estimador
from sklearn.metrics import accuracy_score
import numpy as np
from sklearn.preprocessing import StandardScaler

SEED = 5  #Definindo a ordem dos números aleatórios
np.random.seed(SEED)
#random_stateint, RandomState instance or None, default=None -> Controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls.
raw_treino_x, raw_teste_x, treino_y, teste_y = train_test_split(x, y, test_size = 0.25,stratify = y) #Separando os dados para treino e teste e estipulando o tamanho para o teste, também. 
print("Treinaremos com %d elementos e testaremos com %d elementos" % (len(treino_x),len(teste_x)))

#Trinando o escalador - Treinando um novo processo de escala: Isso o ocorre devido a discrepância entre os eixos: em X, teremos valores de 0 a 100, e em Y de 0 a 30000. Esses algoritmos são muito suscetíveis a escala, e darão menos valor para variações menores, como é o caso de X.
scaler = StandardScaler() #Escalas distintas em features causam um desbalanço no algoritmo
scaler.fit(raw_treino_x)
treino_x = scaler.transform(raw_treino_x)
teste_x = scaler.transform(raw_teste_x)

modelo = SVC()
modelo.fit(treino_x, treino_y)

previsoes = modelo.predict(teste_x) #Predizendo através dos meus dados de teste, após ser treinado com meus dados de treino
taxa_de_acerto = accuracy_score(teste_y, previsoes)* 100 #Imprimir minha taxa de acerto ou "acurácia" - Comparação com os meus dados reais de teste
print("A acurácia foi %.2f" % taxa_de_acerto)

data_x = teste_x[:,0] #-> teste_x, pegando todas as linhas da coluna 0.        #Pegando a primeira coluna usando numpy.
data_y = teste_x[:,1] #-> teste_y, pegando todas as linhas da coluna 1.        #Pegando a primeira coluna usando numpy.

x_min = data_x.min()
x_max = data_x.max()
y_min = data_y.min()
y_max = data_y.max()


pixels = 100
eixo_x = np.arange(x_min, x_max, (x_max - x_min)/pixels)
eixo_y = np.arange(x_min, x_max, (x_max - x_min)/pixels)

xx, yy = np.meshgrid(eixo_x, eixo_y) #Return coordinate matrices from coordinate vectors.Ex.: Plano cartesiano
pontos = np.c_[xx.ravel(), yy.ravel()]#Return a contiguous flattened array


Z = modelo.predict(pontos) #Prevendo para todos os pontos 
Z = Z.reshape(xx.shape)

import matplotlib.pyplot as plt
plt.contourf(xx, yy, Z, alpha=0.3) #Desenhar o contorno de xx e yy baseado nos pontos de Z
plt.scatter(data_x, data_y, c=teste_y, s=5)

plt.contourf(xx, yy, Z, alpha=0.3)
plt.scatter(data_x, data_y, c=teste_y, s=1)

from sklearn.preprocessing import StandardScaler # --> Redefinindo escalas